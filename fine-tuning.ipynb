{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1808590,"sourceType":"datasetVersion","datasetId":989445},{"sourceId":10178011,"sourceType":"datasetVersion","datasetId":6286661},{"sourceId":10178255,"sourceType":"datasetVersion","datasetId":6286868},{"sourceId":212798112,"sourceType":"kernelVersion"}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":2695.631154,"end_time":"2024-12-13T04:05:51.106111","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-13T03:20:55.474957","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{}},"colab":{"provenance":[]}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"74153289","cell_type":"code","source":"pip install transformers datasets torch scikit-learn pandas\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":9.33552,"end_time":"2024-12-13T03:21:07.208840","exception":false,"start_time":"2024-12-13T03:20:57.873320","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:58:23.613788Z","iopub.execute_input":"2024-12-21T04:58:23.614148Z","iopub.status.idle":"2024-12-21T04:58:32.947841Z","shell.execute_reply.started":"2024-12-21T04:58:23.614113Z","shell.execute_reply":"2024-12-21T04:58:32.946762Z"},"id":"74153289","outputId":"1c695e43-b5ee-4c77-cf03-3cc364b6ffbb"},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.1.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.2.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"id":"15c4ac84-e2ba-467c-a908-dee37bc65524","cell_type":"markdown","source":"# Download dataset","metadata":{"id":"15c4ac84-e2ba-467c-a908-dee37bc65524"}},{"id":"0872314a-6121-487f-b58c-2172341d27b2","cell_type":"code","source":"import kagglehub\nimport pandas as pd\n\npath = kagglehub.dataset_download(\"abhi8923shriv/sentiment-analysis-dataset\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:58:32.949959Z","iopub.execute_input":"2024-12-21T04:58:32.950757Z","iopub.status.idle":"2024-12-21T04:58:34.329153Z","shell.execute_reply.started":"2024-12-21T04:58:32.950711Z","shell.execute_reply":"2024-12-21T04:58:34.327901Z"},"id":"0872314a-6121-487f-b58c-2172341d27b2","outputId":"15bc0e73-4307-4d92-d421-e328ffac5f11"},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/sentiment-analysis-dataset\n","output_type":"stream"}],"execution_count":2},{"id":"487dfbe9-dbc0-492a-915c-883610de5f73","cell_type":"code","source":"import pandas as pd\n\ncsv_path = path+\"/train.csv\"\ndata = pd.read_csv(csv_path, encoding='latin1')\nprint(\"\\nSố dòng trong mỗi cột:\")\nprint(len(data[['text', 'sentiment']]))\n\nprint(\"\\nSố dòng thiếu trong mỗi cột:\")\nprint(data[['text', 'sentiment']].isna().sum())\n\ndata_cleaned = data.dropna(subset=['text', 'sentiment'])\n\ndata_cleaned = data_cleaned[data_cleaned['text'].str.strip() != \"\"]\ndata_cleaned = data_cleaned[data_cleaned['sentiment'].str.strip() != \"\"]\n\ndata_cleaned.to_csv(\"cleaned_train.csv\", index=False)\n\n#================================================\n\ncsv_path = path+\"/test.csv\"\ndata = pd.read_csv(csv_path, encoding='latin1')\n\nprint(\"\\nSố dòng thiếu trong mỗi cột:\")\nprint(data[['text', 'sentiment']].isna().sum())\n\ndata_cleaned = data.dropna(subset=['text', 'sentiment'])\n\ndata_cleaned = data_cleaned[data_cleaned['text'].str.strip() != \"\"]\ndata_cleaned = data_cleaned[data_cleaned['sentiment'].str.strip() != \"\"]\n\ndata_cleaned.to_csv(\"cleaned_test.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:58:34.330633Z","iopub.execute_input":"2024-12-21T04:58:34.331043Z","iopub.status.idle":"2024-12-21T04:58:34.721817Z","shell.execute_reply.started":"2024-12-21T04:58:34.330979Z","shell.execute_reply":"2024-12-21T04:58:34.720983Z"},"id":"487dfbe9-dbc0-492a-915c-883610de5f73","outputId":"09d89a3b-b7ce-4fde-c654-894508b06bae"},"outputs":[{"name":"stdout","text":"\nSố dòng trong mỗi cột:\n27481\n\nSố dòng thiếu trong mỗi cột:\ntext         1\nsentiment    0\ndtype: int64\n\nSố dòng thiếu trong mỗi cột:\ntext         1281\nsentiment    1281\ndtype: int64\n","output_type":"stream"}],"execution_count":3},{"id":"556d2efa","cell_type":"code","source":"import pandas as pd\n\n# Đọc file CSV\ndf = pd.read_csv('/kaggle/working/cleaned_train.csv', encoding='latin-1')\ntdf = pd.read_csv('/kaggle/working/cleaned_test.csv', encoding='latin-1')\n\nprint(df.head())\n\nprint(df.columns)\n\ndf = df[['text', 'sentiment']]\ntdf = tdf[['text', 'sentiment']]\n\ndef convert_sentiment_label(x):\n    if x == 'positive':\n        return 1\n    elif x == 'negative':\n        return 0\n    else:\n        return 2\n\ndf['sentiment'] = df['sentiment'].apply(convert_sentiment_label)\ntdf['sentiment'] = tdf['sentiment'].apply(convert_sentiment_label)\n\ndf['text'] = df['text'].astype(str)\ntdf['text'] = tdf['text'].astype(str)\n\ntrain_texts = df['text']\ntrain_labels = df['sentiment']\nval_texts= tdf['text']\nval_labels = tdf['sentiment']\n\nprint(\"Train size:\", len(train_texts))\nprint(\"Test size:\", len(val_texts))","metadata":{"papermill":{"duration":0.898736,"end_time":"2024-12-13T03:21:08.110457","exception":false,"start_time":"2024-12-13T03:21:07.211721","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:58:34.724221Z","iopub.execute_input":"2024-12-21T04:58:34.724585Z","iopub.status.idle":"2024-12-21T04:58:34.845778Z","shell.execute_reply.started":"2024-12-21T04:58:34.724544Z","shell.execute_reply":"2024-12-21T04:58:34.844883Z"},"id":"556d2efa","outputId":"46f590dd-a6fd-4da4-af5c-d98141ec353d"},"outputs":[{"name":"stdout","text":"       textID                                               text  \\\n0  cb774db0d1                I`d have responded, if I were going   \n1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n2  088c60f138                          my boss is bullying me...   \n3  9642c003ef                     what interview! leave me alone   \n4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n\n                         selected_text sentiment Time of Tweet Age of User  \\\n0  I`d have responded, if I were going   neutral       morning        0-20   \n1                             Sooo SAD  negative          noon       21-30   \n2                          bullying me  negative         night       31-45   \n3                       leave me alone  negative       morning       46-60   \n4                        Sons of ****,  negative          noon       60-70   \n\n       Country  Population -2020  Land Area (KmÂ²)  Density (P/KmÂ²)  \n0  Afghanistan          38928346          652860.0                60  \n1      Albania           2877797           27400.0               105  \n2      Algeria          43851044         2381740.0                18  \n3      Andorra             77265             470.0               164  \n4       Angola          32866272         1246700.0                26  \nIndex(['textID', 'text', 'selected_text', 'sentiment', 'Time of Tweet',\n       'Age of User', 'Country', 'Population -2020', 'Land Area (KmÂ²)',\n       'Density (P/KmÂ²)'],\n      dtype='object')\nTrain size: 27480\nTest size: 3534\n","output_type":"stream"}],"execution_count":4},{"id":"90453d78","cell_type":"code","source":"from transformers import BertTokenizer, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')\nprint(\"Tokenizer loaded successfully!\")\n\ndef tokenize_function(texts):\n    return tokenizer(texts, padding='max_length', truncation=True, max_length=220, return_tensors=\"pt\")\n\ntrain_encodings = tokenize_function(list(train_texts))\nval_encodings = tokenize_function(list(val_texts))\n","metadata":{"papermill":{"duration":14.781094,"end_time":"2024-12-13T03:21:22.894256","exception":false,"start_time":"2024-12-13T03:21:08.113162","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:58:34.846777Z","iopub.execute_input":"2024-12-21T04:58:34.847066Z","iopub.status.idle":"2024-12-21T04:58:46.553959Z","shell.execute_reply.started":"2024-12-21T04:58:34.847038Z","shell.execute_reply":"2024-12-21T04:58:46.553214Z"},"colab":{"referenced_widgets":["11f226f64fe543728aeaae0fb3683cb8","b9581f40b4a843e78c17a0c6af87c0eb","82d0ac8ea938431da92fdde3518e4622","61d43e19855741939b2f57d9199d056c"]},"id":"90453d78","outputId":"971acb3b-9861-44fb-f74d-f50f10d3c68f"},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81abfe597afb4a4dac03e5eac78c4309"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a83dcc6bf34840ec86371cb8539d836f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43492f113cb447e5888748d1b7c16317"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb55cdb33fc24e07bbc56d85014e4ca0"}},"metadata":{}},{"name":"stdout","text":"Tokenizer loaded successfully!\n","output_type":"stream"}],"execution_count":5},{"id":"a6efd630","cell_type":"code","source":"import torch\n\nclass SentimentDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\ntrain_dataset = SentimentDataset(train_encodings, train_labels)\nval_dataset = SentimentDataset(val_encodings, val_labels)\nprint('run complete')\n","metadata":{"papermill":{"duration":0.016251,"end_time":"2024-12-13T03:21:22.915710","exception":false,"start_time":"2024-12-13T03:21:22.899459","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:58:46.555163Z","iopub.execute_input":"2024-12-21T04:58:46.555692Z","iopub.status.idle":"2024-12-21T04:58:46.562778Z","shell.execute_reply.started":"2024-12-21T04:58:46.555652Z","shell.execute_reply":"2024-12-21T04:58:46.561819Z"},"id":"a6efd630","outputId":"c39d68bb-a488-4949-f3d5-f5663f5b6780"},"outputs":[{"name":"stdout","text":"run complete\n","output_type":"stream"}],"execution_count":6},{"id":"2829b0f3-6f14-49e2-81b2-22651fafd239","cell_type":"code","source":"!pip install --upgrade peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:58:46.563740Z","iopub.execute_input":"2024-12-21T04:58:46.563951Z","iopub.status.idle":"2024-12-21T04:58:55.228607Z","shell.execute_reply.started":"2024-12-21T04:58:46.563929Z","shell.execute_reply":"2024-12-21T04:58:55.227672Z"},"id":"2829b0f3-6f14-49e2-81b2-22651fafd239","outputId":"5772ee5e-f6a6-4ac8-8505-ae64a425d924"},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.1.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.26.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2024.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.14.0\n","output_type":"stream"}],"execution_count":7},{"id":"684c007c","cell_type":"code","source":"from peft import PrefixEncoder, PrefixTuningConfig, get_peft_model\nfrom transformers import BertForSequenceClassification\nimport torch\nfrom torch import nn\n\nconfig = PrefixTuningConfig(\n    peft_type=\"PREFIX_TUNING\",\n    task_type=\"SEQ_CLS\",\n    num_virtual_tokens=50,\n    token_dim=1024,\n    num_transformer_submodules=1,\n    num_attention_heads=16,\n    encoder_hidden_size=1024\n)","metadata":{"papermill":{"duration":14.995537,"end_time":"2024-12-13T03:21:37.914233","exception":false,"start_time":"2024-12-13T03:21:22.918696","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:58:55.230280Z","iopub.execute_input":"2024-12-21T04:58:55.230971Z","iopub.status.idle":"2024-12-21T04:59:07.377400Z","shell.execute_reply.started":"2024-12-21T04:58:55.230926Z","shell.execute_reply":"2024-12-21T04:59:07.376704Z"},"id":"684c007c"},"outputs":[],"execution_count":8},{"id":"205b4011","cell_type":"code","source":"from torch.nn import CrossEntropyLoss\nfrom torch import cuda\nfrom torch.utils.data import DataLoader\nfrom transformers import AdamW\nfrom tqdm import tqdm\nimport time\nimport random\nfrom torch.utils.data import Subset\nfrom sklearn.metrics import accuracy_score\nfrom transformers import BertForSequenceClassification, DistilBertForSequenceClassification\nfrom collections import Counter\n\ndevice = 'cuda' if cuda.is_available() else 'cpu'\ndata_len_list = [100, 200, 300, 400, 500, 1000]\nepochs = 10\nbatch_size = 16\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\ndef EvalModel(model):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            logits = outputs.logits\n            predictions = torch.argmax(logits, dim=-1)\n            all_preds.extend(predictions.cpu().numpy())\n            all_labels.extend(batch['labels'].cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    print(f\"Validation Trained Model Accuracy: {accuracy:.4f}\")\n    print(\"===========================================\")\n\n# Training for many data_length\nfor data_length in data_len_list:\n    model = BertForSequenceClassification.from_pretrained(\n            \"bert-large-uncased\", num_labels=3)\n    model.to(device)\n    model.train()\n    print(\"Training Data Length: \", data_length)\n\n    optimizer = AdamW(model.parameters(), lr=5e-4)\n    #random_indices = random.sample(range(len(train_dataset)), data_length)\n    subset_indices = list(range(data_length))\n    train_subset = Subset(train_dataset, subset_indices)\n    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n    \n    label_counts = Counter()\n    for batch in train_loader:\n        labels = batch['labels']\n        label_counts.update(labels.tolist())\n    for label, count in label_counts.items():\n        print(f\"Label {label}: {count} samples\")\n\n    for epoch in range(epochs):\n        start_time = time.time()\n        total_loss = 0\n        correct_predictions = 0\n        total_samples = 0\n        for batch in train_loader:\n            optimizer.zero_grad()\n            batch = {k: v.to(device) for k, v in batch.items()}\n            labels = batch[\"labels\"]\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1)\n            correct_predictions += torch.sum(preds == labels).item()\n            total_samples += labels.size(0)\n        epoch_time = time.time() - start_time\n        avg_loss = total_loss / len(train_loader)\n        accuracy = correct_predictions / total_samples * 100\n        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}, \"\n              f\"Accuracy: {correct_predictions / total_samples:.4f}, \"\n              f\"Time: {epoch_time:.2f} seconds\")\n\n    # Eval model =====================================================\n    EvalModel(model)\n\n\n\n\n","metadata":{"papermill":{"duration":2632.507881,"end_time":"2024-12-13T04:05:30.425529","exception":false,"start_time":"2024-12-13T03:21:37.917648","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T04:59:07.378444Z","iopub.execute_input":"2024-12-21T04:59:07.378919Z","iopub.status.idle":"2024-12-21T05:35:26.201411Z","shell.execute_reply.started":"2024-12-21T04:59:07.378892Z","shell.execute_reply":"2024-12-21T05:35:26.200295Z"},"id":"205b4011","outputId":"f3d70259-f90c-4bdb-870a-e4e244c08331"},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"445f0a09039f43d28d809805955b94f2"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Training Data Length:  100\nLabel 2: 43 samples\nLabel 0: 33 samples\nLabel 1: 24 samples\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 1.2220860719680786, Accuracy: 0.3800, Time: 7.44 seconds\nEpoch 2/10, Loss: 1.1322765265192305, Accuracy: 0.4000, Time: 6.77 seconds\nEpoch 3/10, Loss: 1.154542360986982, Accuracy: 0.3600, Time: 6.78 seconds\nEpoch 4/10, Loss: 1.1888200896126884, Accuracy: 0.3300, Time: 6.78 seconds\nEpoch 5/10, Loss: 1.0860763788223267, Accuracy: 0.4700, Time: 6.78 seconds\nEpoch 6/10, Loss: 1.1381023951939173, Accuracy: 0.4000, Time: 6.80 seconds\nEpoch 7/10, Loss: 1.1397927829197474, Accuracy: 0.4300, Time: 6.79 seconds\nEpoch 8/10, Loss: 1.0972651072910853, Accuracy: 0.3500, Time: 6.80 seconds\nEpoch 9/10, Loss: 1.221918991633824, Accuracy: 0.3200, Time: 6.82 seconds\nEpoch 10/10, Loss: 1.166099292891366, Accuracy: 0.3100, Time: 6.83 seconds\nValidation Trained Model Accuracy: 0.4046\n===========================================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Training Data Length:  200\nLabel 1: 54 samples\nLabel 0: 64 samples\nLabel 2: 82 samples\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 1.1840519125644977, Accuracy: 0.2950, Time: 13.68 seconds\nEpoch 2/10, Loss: 1.135845069701855, Accuracy: 0.2500, Time: 13.69 seconds\nEpoch 3/10, Loss: 1.141079279092642, Accuracy: 0.3600, Time: 13.70 seconds\nEpoch 4/10, Loss: 1.1228331327438354, Accuracy: 0.3600, Time: 13.70 seconds\nEpoch 5/10, Loss: 1.1268863769677968, Accuracy: 0.3650, Time: 13.69 seconds\nEpoch 6/10, Loss: 1.0730758492763226, Accuracy: 0.4050, Time: 13.68 seconds\nEpoch 7/10, Loss: 1.1848417703921978, Accuracy: 0.3200, Time: 13.69 seconds\nEpoch 8/10, Loss: 1.1188889283400316, Accuracy: 0.3800, Time: 13.66 seconds\nEpoch 9/10, Loss: 1.1442609887856703, Accuracy: 0.3850, Time: 13.68 seconds\nEpoch 10/10, Loss: 1.10909942480234, Accuracy: 0.3500, Time: 13.69 seconds\nValidation Trained Model Accuracy: 0.4046\n===========================================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Training Data Length:  300\nLabel 1: 88 samples\nLabel 0: 83 samples\nLabel 2: 129 samples\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 1.1380485108024196, Accuracy: 0.4067, Time: 20.51 seconds\nEpoch 2/10, Loss: 1.1624367299832796, Accuracy: 0.3267, Time: 20.52 seconds\nEpoch 3/10, Loss: 1.1513260289242393, Accuracy: 0.3667, Time: 20.51 seconds\nEpoch 4/10, Loss: 1.139077089334789, Accuracy: 0.3500, Time: 20.49 seconds\nEpoch 5/10, Loss: 1.1287667468974465, Accuracy: 0.3800, Time: 20.51 seconds\nEpoch 6/10, Loss: 1.1154336364645707, Accuracy: 0.4300, Time: 20.49 seconds\nEpoch 7/10, Loss: 1.1498660476584184, Accuracy: 0.3733, Time: 20.53 seconds\nEpoch 8/10, Loss: 1.1902077982300205, Accuracy: 0.3733, Time: 20.51 seconds\nEpoch 9/10, Loss: 1.1636421053033126, Accuracy: 0.3800, Time: 20.52 seconds\nEpoch 10/10, Loss: 1.1297402162300914, Accuracy: 0.3433, Time: 20.50 seconds\nValidation Trained Model Accuracy: 0.3121\n===========================================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Training Data Length:  400\nLabel 2: 168 samples\nLabel 0: 118 samples\nLabel 1: 114 samples\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 1.1511055469512939, Accuracy: 0.3675, Time: 27.28 seconds\nEpoch 2/10, Loss: 1.1176502156257628, Accuracy: 0.3725, Time: 27.29 seconds\nEpoch 3/10, Loss: 1.1510272359848022, Accuracy: 0.3500, Time: 27.29 seconds\nEpoch 4/10, Loss: 1.1266818428039551, Accuracy: 0.3875, Time: 27.27 seconds\nEpoch 5/10, Loss: 1.1558059024810792, Accuracy: 0.3450, Time: 27.27 seconds\nEpoch 6/10, Loss: 1.1529788589477539, Accuracy: 0.3625, Time: 27.28 seconds\nEpoch 7/10, Loss: 1.1217599248886108, Accuracy: 0.3475, Time: 27.31 seconds\nEpoch 8/10, Loss: 1.1055075216293335, Accuracy: 0.3650, Time: 27.29 seconds\nEpoch 9/10, Loss: 1.1228988027572633, Accuracy: 0.3425, Time: 27.29 seconds\nEpoch 10/10, Loss: 1.116033275127411, Accuracy: 0.3625, Time: 27.28 seconds\nValidation Trained Model Accuracy: 0.4046\n===========================================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Training Data Length:  500\nLabel 0: 154 samples\nLabel 1: 147 samples\nLabel 2: 199 samples\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 1.1816784050315619, Accuracy: 0.3400, Time: 34.17 seconds\nEpoch 2/10, Loss: 1.1359623149037361, Accuracy: 0.3400, Time: 34.18 seconds\nEpoch 3/10, Loss: 1.178428702056408, Accuracy: 0.3440, Time: 34.19 seconds\nEpoch 4/10, Loss: 1.129646709188819, Accuracy: 0.3660, Time: 34.17 seconds\nEpoch 5/10, Loss: 1.1565310955047607, Accuracy: 0.3660, Time: 34.16 seconds\nEpoch 6/10, Loss: 1.1499235536903143, Accuracy: 0.3420, Time: 34.20 seconds\nEpoch 7/10, Loss: 1.1133504770696163, Accuracy: 0.3680, Time: 34.17 seconds\nEpoch 8/10, Loss: 1.1391855869442225, Accuracy: 0.3680, Time: 34.17 seconds\nEpoch 9/10, Loss: 1.1280756201595068, Accuracy: 0.3400, Time: 34.18 seconds\nEpoch 10/10, Loss: 1.1506738606840372, Accuracy: 0.3660, Time: 34.17 seconds\nValidation Trained Model Accuracy: 0.2832\n===========================================\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Training Data Length:  1000\nLabel 0: 295 samples\nLabel 2: 390 samples\nLabel 1: 315 samples\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 1.152097881786407, Accuracy: 0.3600, Time: 68.27 seconds\nEpoch 2/10, Loss: 1.132719001126668, Accuracy: 0.3610, Time: 68.27 seconds\nEpoch 3/10, Loss: 1.1655464683260237, Accuracy: 0.3270, Time: 68.25 seconds\nEpoch 4/10, Loss: 1.1443409512913416, Accuracy: 0.3620, Time: 68.32 seconds\nEpoch 5/10, Loss: 1.174426539549752, Accuracy: 0.3420, Time: 68.27 seconds\nEpoch 6/10, Loss: 1.127492454316881, Accuracy: 0.3680, Time: 68.27 seconds\nEpoch 7/10, Loss: 1.138550779176137, Accuracy: 0.3360, Time: 68.28 seconds\nEpoch 8/10, Loss: 1.1333500742912292, Accuracy: 0.3510, Time: 68.30 seconds\nEpoch 9/10, Loss: 1.1542118589083354, Accuracy: 0.3630, Time: 68.31 seconds\nEpoch 10/10, Loss: 1.1266653216074383, Accuracy: 0.3600, Time: 68.32 seconds\nValidation Trained Model Accuracy: 0.4046\n===========================================\n","output_type":"stream"}],"execution_count":9}]}